{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Data mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = webdriver.Edge()\n",
    "driver.get(\"https://www.daft.ie/property-for-sale/ireland\")\n",
    "result = driver.find_element(By.CLASS_NAME,\"styles__SearchH1-sc-1t5gb6v-3\")\n",
    "num = result.text.split()[0]\n",
    "num = num.replace(\",\",\"\")\n",
    "num =2000\n",
    "count =0\n",
    "all_links = []\n",
    "while(count<= int(num)):\n",
    "    driver.get(\"https://www.daft.ie/property-for-sale/ireland?from=\"+str(count)+\"&pageSize=20\")\n",
    "    a_tags = driver.find_elements(By.XPATH,\"//ul[@class='SearchPagestyled__SearchResults-v8jvjf-3 hzMJok']/li/a\")\n",
    "    all_links.extend([a_tag.get_attribute('href') for a_tag in a_tags])\n",
    "    count += 20\n",
    "\n",
    "\n",
    "filename = \"links.txt\"\n",
    "print(filename)\n",
    "\n",
    "with open(filename, 'wb') as f:\n",
    "    print(\"Saving to\", filename)\n",
    "    for link in all_links:\n",
    "        print(link)\n",
    "        string_link = str(link)+ \"\\n\"\n",
    "        f.write(string_link.encode('utf-8'))\n",
    "    f.close()\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open('links.txt', 'r') as f:\n",
    "    urls = f.readlines()\n",
    "    f.close()\n",
    "    \n",
    "def get_element(soup, key, value, tag,default='N/A'):\n",
    "    try:\n",
    "        if tag == 'ul':\n",
    "            data = soup.find(tag,{key:value}).find('li')\n",
    "        else:\n",
    "            data = soup.find(tag,{key:value})\n",
    "            \n",
    "        if data:\n",
    "            return data.text.strip()\n",
    "        return default\n",
    "    except:\n",
    "        return default\n",
    "    \n",
    "def get_elements(soup, key, value, tag,default='N/A',index_data=1):\n",
    "    try:\n",
    "        datas = soup.find_all(tag,{key:value})\n",
    "        if len(datas)>0:\n",
    "            if(index_data == 0):\n",
    "                for i in range(len(datas)):\n",
    "                    datas[i] = datas[i].text.strip()\n",
    "                return datas\n",
    "            else:\n",
    "                return datas[index_data-1].text.strip()\n",
    "        return default\n",
    "    except:\n",
    "        return default \n",
    "\n",
    "data = []\n",
    "driver = webdriver.Edge()\n",
    "for url in urls:\n",
    "    url = url.replace(\"\\n\",\"\")\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    address = get_element(soup,'data-testid','address','h1')\n",
    "    price = get_element(soup,'class','TitleBlock__StyledCustomHeading-sc-1avkvav-5 blbeVq','h2')\n",
    "    beds = get_element(soup,'data-testid','beds','p')\n",
    "    baths = get_element(soup,'data-testid','baths','p')\n",
    "    floor_area = get_element(soup,'data-testid','floor-area','p')\n",
    "    property_type = get_element(soup,'data-testid','property-type','p')\n",
    "    selling_method = get_element(soup,'class','styles__InfoSection-sc-15fxapi-7 ikMOXo','ul')\n",
    "    condition = get_element(soup,'class','Statistics__StyledValue-sc-15tgae4-2 MCfSO','p')\n",
    "    date_statistics = get_elements(soup,'class','Statistics__StyledLabel-sc-15tgae4-1 iDjRee','p',index_data=1)\n",
    "    viewer = get_elements(soup,'class','Statistics__StyledLabel-sc-15tgae4-1 iDjRee','p',index_data=2)\n",
    "    property_features_or_facilities = get_elements(soup,'class','PropertyDetailsList__PropertyDetailsListItem-sc-1cjwtjz-1 hqJwsU','li')\n",
    "    data.append({'address':address,'price':price,'beds':beds,'baths':baths,'floor_area':floor_area,'property_type':property_type,'selling_method':selling_method,'condition':condition,'date_statistics':date_statistics,'viewer':viewer,'property_features_or_facilities':property_features_or_facilities})\n",
    "    print(\"Count: \",len(data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filename = \"data.csv\"\n",
    "fieldnames = ['address','price','beds','baths','floor_area','property_type','selling_method','condition','date_statistics','viewer','property_features_or_facilities']\n",
    "with open(filename,'w',newline='',encoding='utf-8') as f:\n",
    "    print(\"Saving to \",filename)\n",
    "    csv_writer = csv.DictWriter(f,fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "    csv_writer.writerows(data)\n",
    "    f.close()\n",
    "    \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be scrabbed from the website on obtaining the property links and write it into links.txt file. Then load each of the property link to get the detailed information and write it into the data.csv file"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
